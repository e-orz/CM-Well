spray.can.client {
  response-chunk-aggregation-limit = 0
  parsing {
    max-chunk-size = 5m
    max-content-length = 360m
  }
}

akka {
  log-dead-letters = 2
  log-dead-letters-during-shutdown = on
  # log-config-on-start = on
  #  jvm-exit-on-fatal-error = on

  loggers = ["akka.event.slf4j.Slf4jLogger"]
  #akka is configured to log in DEBUG level. The actual level is determined by logback
  loglevel = "DEBUG"
  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"

  http.host-connection-pool {
    max-connections = 20
    max-open-requests = 128
    client {
      user-agent-header = DcSync using akka-http/${akka.version}
      parsing.max-content-length = 360M
    }
  }
}

data-tools {
  akka {
    http.host-connection-pool {
      max-connections = 20
      max-open-requests = 128
      client {
        user-agent-header = cmwell-data-tools using akka-http/${akka.version}
      }
    }
  }
}

cmwell {
  dc {
    clusterName = cm-well
    hostName = 127.0.0.1
    seedNodes = 127.0.0.1
    port = 0
    target = "127.0.0.1:9000"
    # FileInfotons contain a possibly large data chunk (25M) which
    # is inflated because it is rendered to Base64 (3:4 ratio)
    # and also need to take into account the other nq statement parts.
    maxStatementLength = 35M
    pull {
      initialTsvRetryCount = 10
      bulkTsvRetryCount = 2
      #createConsumeLengthHint = 500000 #Optional parameter to set the lengh-hint for the create-consume
      consumeFallbackDuration = 5 minutes # time of degradation from bulk to consume
      #tsvBufferSize = 1 # actually cancelled - the buffer is feature is got from maxTotalInfotonCountAggregatedForRetrieve
      #delayInSecondsBetweenNoContentRetries = 30 #actually not used due to what seems to be a bug in akka http
      maxRetrieveInfotonCount = 25
      maxRetrieveByteSize = 1MB
      maxTotalInfotonCountAggregatedForRetrieve = 100000
      retrieveParallelism = 20
      initialBulkRetrieveRetryCount = 0
      maxBulkRetrieveRetryDelay = 1200 milliseconds # The delay will increment for each retry until this delay
      initialSingleRetrieveRetryCount = 6
      maxSingleRetrieveRetryDelay = 1200 milliseconds # The delay will increment for each retry until this delay
      retrieveRetryQueueSize = 25
    }
    push {
      maxIngestInfotonCount = 25
      maxIngestByteSize = 1MB
      maxTotalInfotonCountAggregatedForIngest = 100
      initialBulkIngestRetryCount = 0
      initialSingleIngestRetryCount = 5
      gzippedIngest = false
      ingestParallelism = 20
      ingestRetryQueueSize = 25
      ingestRetryDelay = 1 second
      ingestServiceUnavailableDelay = 10 second
    }
  }
  clusterName = {{cmwell.clusterName}}
  grid {
    clusterName = ${cmwell.clusterName}
    dmap.persistence.data-dir = "{{cmwell.grid.dmap.persistence.data-dir}}"
    bind.host = "{{cmwell.grid.bind.host}}"
    bind.port = {{cmwell.grid.bind.port}}
    seeds = "{{cmwell.grid.seeds}}"
    min-members = {{cmwell.grid.min-members}}
    monitor.port = {{cmwell.grid.monitor.port}}
  }
  downloader.consumer.consume-fetch-size = 100
  downloader.consumer.prefetch-buffer-size = 1000
  triggeredProcessor.infoton-group-size = 200


  dc.target = "{{cmwell.dc.target}}"
}

ctrl {
  home = "{{ctrl.home}}"
  pingIp = "{{ctrl.pingIp}}"
  externalHostName = "{{ctrl.externalHostName}}"
}

metrics {
  reportMetricsJMX = false
  reportMetricsGraphite = false
  reportMetricsSlf4j = false
}

irwServiceDao {
  hostName = "{{irwServiceDao.hostName}}" #localhost
  clusterName = "{{irwServiceDao.clusterName}}" #cmwell
  keySpace2 = data2
}

dcaUser.token = eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJkY2EiLCJleHAiOjQ2Mzg5MDI0MDAwMDAsInJldiI6MX0.mxAkZMgzVSnb1LPRM1hEBnxX0crhisNAdqRG9iiP3JU
dcaUser.token = ${?DCA_USER_TOKEN}
